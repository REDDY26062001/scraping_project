# -*- coding: utf-8 -*-
"""news_article.csv

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sTcTb2otNiVOcQpfFEclmYMs60p-xIYa
"""

import requests
from bs4 import BeautifulSoup
import csv
import time

# BBC URL
bbc_url = "https://www.bbc.com/news/live/c5yw80xv4wjt"
response = requests.get(bbc_url)
soup = BeautifulSoup(response.content, 'html.parser')

# Fetching all headlines, publication dates, and URLs
headlines = soup.find_all('h3')
dates = soup.find_all('time')
anchors = soup.find_all('a', href=True)  # Find all anchor tags with href attributes

# Create lists to store headline texts, publication dates, and URLs
headline_list = [headline.text for headline in headlines]
date_list = [date['datetime'] if date.has_attr('datetime') else "Unknown Date" for date in dates]
# Filter the anchor tags to get valid URLs, assuming the parent of <h3> is <a>
url_list = [a['href'] if a['href'].startswith('https') else 'https://www.bbc.com' + a['href'] for a in anchors]

# Set the source (since we're using BBC, it's constant)
source = "BBC News"

# Print each headline along with the publication date, source, and URL
for i, headline in enumerate(headlines):
    publication_date = date_list[i] if i < len(date_list) else "Unknown Date"
    url = url_list[i] if i < len(url_list) else "URL not available"
    print(f"{headline.text} (Published: {publication_date}, Source: {source}, URL: {url})\n")
    time.sleep(1)


def generate_summary(headlines_list, dates_list, url_list, source):
    # Example approach: summarizing by printing a certain number of headlines, their dates, source, and URL
    if len(headlines_list) == 0:
        return "No headlines to summarize."

    summary = f"Total headlines: {len(headlines_list)}\n"
    summary += "Key headlines with publication dates, source, and URLs:\n"

    # Print first 3 headlines, dates, source, and URLs for simplicity in summary
    for i, headline in enumerate(headlines_list[:3]):
        publication_date = dates_list[i] if i < len(dates_list) else "Unknown Date"
        url = url_list[i] if i < len(url_list) else "URL not available"
        summary += f"{i + 1}. {headline} (Published: {publication_date}, Source: {source}, URL: {url})\n"

    return summary


# Print the summary with dates, source, and URLs
summary = generate_summary(headline_list, date_list, url_list, source)
print("\nSummary:\n" + summary)

import requests
from bs4 import BeautifulSoup
import csv
import time

# CNN URL
cnn_url = "https://edition.cnn.com/world/live-news/israel-gaza-lebanon-war-10-10-24/index.html"
response = requests.get(cnn_url)
soup = BeautifulSoup(response.content, 'html.parser')

# Fetching all headlines, publication dates, and URLs
# CNN typically uses <h2> or <h3> for headlines, and <time> for publication dates
headlines = soup.find_all('h2')  # Check the appropriate tag for CNN headlines
dates = soup.find_all('time')  # CNN might also use <time> for publication dates
anchors = soup.find_all('a', href=True)  # Find all anchor tags with href attributes

# Create lists to store headline texts, publication dates, and URLs
headline_list = [headline.text.strip() for headline in headlines]  # Strip to clean extra whitespace
date_list = [date['datetime'] if date.has_attr('datetime') else "Unknown Date" for date in dates]
# Filter the anchor tags to get valid URLs, ensure valid URLs for CNN
url_list = [a['href'] if a['href'].startswith('https') else 'https://edition.cnn.com' + a['href'] for a in anchors]

# Set the source (since we're using CNN, it's constant)
source = "CNN News"

# Print each headline along with the publication date, source, and URL
for i, headline in enumerate(headlines):
    publication_date = date_list[i] if i < len(date_list) else "Unknown Date"
    url = url_list[i] if i < len(url_list) else "URL not available"
    print(f"{headline.text.strip()} (Published: {publication_date}, Source: {source}, URL: {url})\n")
    time.sleep(1)


def generate_summary(headlines_list, dates_list, url_list, source):
    # Example approach: summarizing by printing a certain number of headlines, their dates, source, and URL
    if len(headlines_list) == 0:
        return "No headlines to summarize."

    summary = f"Total headlines: {len(headlines_list)}\n"
    summary += "Key headlines with publication dates, source, and URLs:\n"

    # Print first 3 headlines, dates, source, and URLs for simplicity in summary
    for i, headline in enumerate(headlines_list[:3]):
        publication_date = dates_list[i] if i < len(dates_list) else "Unknown Date"
        url = url_list[i] if i < len(url_list) else "URL not available"
        summary += f"{i + 1}. {headline} (Published: {publication_date}, Source: {source}, URL: {url})\n"

    return summary


# Print the summary with dates, source, and URLs
summary = generate_summary(headline_list, date_list, url_list, source)
print("\nSummary:\n" + summary)

!pip install nltk spacy scikit-learn
!python -m spacy download en_core_web_sm

import spacy
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn import metrics

# Load spaCy model for NLP processing
nlp = spacy.load("en_core_web_sm")
nltk.download('stopwords')
from nltk.corpus import stopwords

# Sample dataset (Replace with real dataset)
articles = [
    "The president signed a new bill into law today.",  # Politics
    "The latest technology trends include AI, ML, and cloud computing.",  # Technology
    "The soccer match ended with a score of 3-1.",  # Sports
    # Add more articles...
]
labels = ['Politics', 'Technology', 'Sports']  # Corresponding labels

# Preprocessing function using spaCy
def preprocess(text):
    doc = nlp(text)
    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]
    return ' '.join(tokens)

# Preprocess all articles
preprocessed_articles = [preprocess(article) for article in articles]

# Define a TF-IDF vectorizer and Naive Bayes classifier
model = make_pipeline(TfidfVectorizer(), MultinomialNB())

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(preprocessed_articles, labels, test_size=0.3, random_state=42)

# Train the model
model.fit(X_train, y_train)

# Predict on test data
y_pred = model.predict(X_test)

# Evaluation
print("Accuracy:", metrics.accuracy_score(y_test, y_pred))
print("Classification Report:\n", metrics.classification_report(y_test, y_pred))

# Example prediction on a new article
new_article = "The team won the championship after a thrilling final match."
preprocessed_article = preprocess(new_article)
print("Predicted Topic:", model.predict([preprocessed_article])[0])



pip install fastapi uvicorn sqlalchemy pydantic

from fastapi import FastAPI, HTTPException, Query
from sqlalchemy.orm import Session
from fastapi import Depends
from database import SessionLocal, engine
import models, crud, schemas

# Create the database tables
models.Base.metadata.create_all(bind=engine)

app = FastAPI()

# Dependency to get the database session
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

@app.get("/articles", response_model=list[schemas.Article])
def get_articles(category: str = None, start_date: str = None, end_date: str = None, db: Session = Depends(get_db)):
    return crud.get_articles(db, category=category, start_date=start_date, end_date=end_date)

@app.get("/articles/{article_id}", response_model=schemas.Article)
def get_article_by_id(article_id: int, db: Session = Depends(get_db)):
    article = crud.get_article_by_id(db, article_id)
    if not article:
        raise HTTPException(status_code=404, detail="Article not found")
    return article

@app.get("/search", response_model=list[schemas.Article])
def search_articles(keyword: str, db: Session = Depends(get_db)):
    return crud.search_articles(db, keyword)

# ipython-input-20-e6115b6a95bc

from fastapi import FastAPI, HTTPException, Query
from sqlalchemy.orm import Session
from fastapi import Depends
from database import SessionLocal, engine # Import from database.py
import models, crud, schemas

# ... rest of your code

# database.py
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

# Replace with your database connection string, specifying the actual port number
# For example, if your PostgreSQL database is running on port 5432:
DATABASE_URL = "postgresql://user:password@host:5432/database"

engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

Base = declarative_base()